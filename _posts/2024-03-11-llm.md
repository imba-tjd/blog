# LLM

* FP32：就是普通的float。FP16：精度稍差，数字范围变为±65000。BF16：精度更差，但保留了FP32的数字范围，只有3060后才支持，IntelXeon3rd也支持。混合精度：平常用FP16，在加法用32。TF32：只有A100支持
  * torch_dtype=torch.bfloat16：pytorch在不指定torch_dtype时会默认转换到float32，一般不用此行为
  * 大多数游戏GPU，FP64的性能是FP32的1/32

## 模型

* 排行榜
  * 英文
    * https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard 可以筛选大小
    * 包含私有模型的榜：https://chat.lmsys.org/?leaderboard
    * https://www.vellum.ai/llm-leaderboard
  * 中文：https://cevalbenchmark.com/static/leaderboard_zh.html https://www.superclueai.com/
  * 代码：https://paperswithcode.com/sota/code-generation-on-humaneval
  * https://ollama.com/library 选MostPopular
* 英文：llama mistral gemma。小模型：phi
* 中文：Qwen2、GLM4（9B很慢）。XuanYuan-6B、BlueLM-7B、Baichuan2-7B-Chat、Yi-6B。小模型：MiniCPM-2B
* 代码：deepseek-coder、dolphin-2_6-phi-2（dolphin基于starcoder2）。Code-Qwen
* 选好后找对应的GGUF版，下Q5_K_M或Q4_K_M，主要靠 https://hf.co/TheBloke
* 原始模型只是续写。Instruct版每个模型有不同的prompt模板，主流是ChatML

## GUI

* https://jan.ai/ 开源桌面版本。导入后在Model和Assistant的三个点处修改全局设置
* text-generation-webui
* https://github.com/h2oai/h2o-llmstudio 开发比较初期但有公司支持
* https://chatboxai.app/
* 闭源：https://lmstudio.ai/ https://faraday.dev/ https://msty.app/
* gpt4all：不能选本地下载的
* https://github.com/lobehub/lobe-chat/blob/main/README.zh-CN.md
* https://github.com/lencx/Noi

## PromptEngineering

* https://platform.openai.com/examples
* https://github.com/dair-ai/Prompt-Engineering-Guide
* https://github.com/f/awesome-chatgpt-prompts https://github.com/PlexPt/awesome-chatgpt-prompts-zh
* https://learnprompting.org/zh-Hans/docs/intro
* https://www.promptingguide.ai/zh
* https://github.com/f/awesome-chatgpt-prompts
* In the context of xxx, 实际问题

## 开发框架

* https://github.com/mudler/LocalAI
* Langchain：工具包，包括RAG、Agent等
  * langchain-huggingface：https://huggingface.co/blog/zh/langchain
* RAG检索增强生成能力：获取外部数据解决幻觉、无法获取实时更新、私有数据。处理框架：quivr、llamaindex、dify。其实是先用外部向量数据库处理自己数据，提问时先从数据库里取出相关内容附加到prompt里，不会更改参数

## transformers

* transformers accelerate(device_map) bitsandbytes(load_in_4bit)
* torch：linux版默认有cuda，装cpu版可用conda install pytorch-cpu；Win默认没有
* datasets
  * load_dataset(ds_id, split=train、'validation[:10]')、ds['train'、'validation']
* evaluate
* optimum
  * BetterTransformer：model.to_bettertransformer() free-lunch加速，支持CPU和GPU
  * 对auto-gptq的封装：gptq.GPTQQuantizer

### 推理

* pipeline支持的任务：https://hf.co/docs/transformers/main/zh/task_summary

```py
from transformers import pipeline

model_id = '本地模型文件夹' # 也称为checkpoint
pipe = pipeline('text-generation', model_id, device_map='auto', load_in_4bit=True) # 可任意指定其它参数，会应用到后续调用中。其它支持的task见quicktour
# pipe()的第一个参数是inputdata，能接受迭代器返回迭代器；此时一般还会调整batch_size
# 在线量化标准用法：qconf = BitsAndBytesConfig(load_in_4bit=True); pipeline(model_kwargs={"quantization_config": qconf})
# TODO: 测试不加任务名

user_input = 'Who are you'
message = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': user_input}
]

outputs = pipe(message, max_new_tokens=256,
    do_sample=True, temperature=0.6, top_p=0.9,
)

o = outputs[0]['generated_text'][-1]['content']

# 继续对话
chat = outputs[0]['generated_text']
chat.append({'role': 'user', 'content': user_input})

# 手动创建使用
model = AutoModelForCausalLM.from_pretrained()
tokenizer = AutoTokenizer.from_pretrained()
formatted_chat = pipe.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(formatted_chat 或 input_text, return_tensors="pt", add_special_tokens=False) # 上一步用tokenize=True则相当于本步
outputs = model.generate(**inputs, max_new_tokens=99)
decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)

# 界面
import gradio as gr
gr.Interface.from_pipeline(pipe).launch()
```

## 微调

* 按规模：PEFT(Parameter Efficient Fine Tuning)低参微调，Full Parameter Fine Tuning全参微调
* 按训练流程：SFT(Supervised Fine Tuning)有监督微调，IT(Instruction Tuning)指令微调是SFT的特殊形式，RLHF(Reinforcemnet Learning Human Feedback)人类反馈强化学习，包括DPO直接策略优化，PPO近端策略优化。不改变参数：In-Context Learning。SFT和RLHF又称“对齐Alignment”
* 教程：https://hf.co/docs/transformers/main/zh/training https://datasciencedojo.com/blog/fine-tuning-llms/ https://www.kaggle.com/code/hhoang41/llama-2-fine-tuning-using-qlora https://github.com/huggingface/transformers/issues/31332 https://generativeai.pub/a-beginners-guide-to-fine-tuning-mixtral-instruct-model-7f6a30aacf61 https://discuss.huggingface.co/t/fine-tune-and-then-successfully-awq-quantize/73239 https://gist.github.com/younesbelkada/f48af54c74ba6a39a7ae4fd777e72fe8
* 工具框架
  * LLaMA-Factory、QLoRA适合小内存（就是peft+bitsandbytes，好像现在AWQ也支持了）、unsloth、peft(hf出的, LoraConfig, PeftModel)
  * LoRA能单独输出微调的部分，之后与基础的合并，方便已有基础镜像的人快速获得微调后的
  * axolotl：简化微调流程
  * TRL：一站式解决SFT(hf，SFTTrainer接受LoraConfig)、RM(Reward Modeling)、PPO，能自动将数据集格式化为chatml格式
  * torchtune：pytorch官方的微调框架，但目前只支持llama mistral gemma，7B+QLoRA消耗8.5G VRAM
  * https://github.com/intel/intel-extension-for-transformers

```py
from peft import get_peft_model, LoraConfig, TaskType
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
model = get_peft_model(model, peft_config)
训练
model.save_pretrained("output_dir") # 只会保存经过训练的增量PEFT权重
# 加载增量权重用于推理
from peft import PeftModel, PeftConfig
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForXXX.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
```

## 量化

* 分类
  * 训练后量化(Post Training Quantization, PTQ)：适度地使用一些资源来量化预训练好的模型
  * 量化感知训练(Quantization Aware Training, QAT)：在训练或进一步微调之前执行量化。如LoRA、P-Tuning、
* 效果：AWQ > GPTQ > bitsandbytes(bnb)。基本都允许配合PEFT
* GPTQ(AutoGPTQ, auto-gptq)采用int4/fp16 (W4A16)的混合量化方案，模型权重被量化为int4，而激活值则保留在float16。在推理阶段，模型权重被动态地反量化回float16。也支持int8
* AWQ：仅有int4
* quanto：HF出的易于使用的量化工具包，兼容torch.compile()，可在线量化，支持CPU；不支持PEFT，不支持保存；感觉对于推理完胜bnb。也是Optimum的后端
* bnb：可用于非文本（多模态）模型，可在线量化（On the fly）无需先单独转换（但也可以保存）。简单，零样本
* device_map='auto' 会自动选用cpu或cuda，且显存不够时会自动用内存

## 其它流程

* 预训练：无标注、无监督，大量token，“下一个单词预测”任务
* 部署：vllm SkyPilot LightLLM openvino
* 加速：DeepSpeed
* Agent：https://github.com/Significant-Gravitas/AutoGPT
* 单机训练LLM：https://github.com/hpcaitech/ColossalAI
* 教程：https://github.com/mlabonne/llm-course
* 利用自然语言生成项目：https://github.com/gpt-engineer-org/gpt-engineer

## HF Hub

* 下载的模型默认储存到 ~/.cache/huggingface/hub。更改：HF_HUB_CACHE
* 用CLI下载：huggingface-cli download model_id；也有py库。支持HF_ENDPOINT。默认断点续传
* 带宽大时更换下载内核：huggingface_hub[hf_transfer]、HF_HUB_ENABLE_HF_TRANSFER=1
* HF_HUB_OFFLINE=1

### HF镜像

* https://hf-mirror.com
* https://modelscope.cn/models
* https://aliendao.cn
* https://aifasthub.com/
* https://ai.gitee.com/

## llama.cpp

* 纯CPU推理，下avx2版本
* 运行：./main -m 模型.gguf -p "prompt"
  * 交互模式：--interactive-first -r "[User]" -p "[System]You are a helpful assistant." --in-prefix "[User]" --in-suffix "[AI]"
  * -n要输出的token数，默认-1无限适合Chat
  * -c context length不同模型不同，默认512，llama2可用4096
    * --keep 保留最初的token数
  * --color 用不同颜色区分用户输入和模型输出
  * -t 要使用的线程，似乎默认等于当前CPU线程数且最大10，推荐改为物理核心数
  * --prompt-cache cache.bin 缓存最初的状态加快启动
  * -f 从文件中读取prompt
  * --temp 0.8、--top-p 0.9 【默】调小会更保守更有确定性，调大会有创意更发散。一般只调其中一个。例如对于答案是唯一确定的场合（翻译）temp用0-0.3，写文章用0.5，写广告用0.7。OpenAI具有frequency_penalty和presence_penalty，默认为0；后者是一次性的，增大则会减少输出已存在过的词的概率，有助于讨论新主题；前者惩罚频繁使用的词，但仍可能从AAA变到ABABAB
  * --repeat-penalty 1.0即禁用 --no-penalize-nl 作者说对于Chat模型不应启用重复惩罚
* example目录、prompts目录
* llama-bench：测试512个长度的提示词的处理时间、生成128个token所需时间
* 相关项目
  * 对用户更友好的封装：ollama run。可以自动利用显卡。llama.cpp也可以但要手动指定 -ngl 层数 控制把多少层放到GPU里。但实际上即使只有1层放到内存里也会造成重大性能损失
  * 与模型一起封装成可执行的单文件：https://github.com/Mozilla-Ocho/llamafile
  * 另一款仅支持HF模型的聊天工具：https://github.com/foldl/chatllm.cpp
* CPU微调：https://rentry.org/cpu-lora
  * 可以微调具有llama架构的gguf
* 转换模型到GGUF：先转换成FP16，再量化，看docs/HOWTO-add-model.md。未测试：convert.py BaseModel --outtype F16 --outfile Base.gguf --concurrency N; quantize.exe Base.gguf Out.Q5_K_M.gguf Q5_K_M
* transformers也能加载gguf，会先解量化到fp32，再转换到pytorch的权重类型

## 免费GPU资源

* Google Colab：没有具体的说明，但是有免费的
* kaggle：NVIDIA TESLA P100 GPU 30 hours/week
* cloudflare ai：10,000 Neurons per day 只有预设的几个模型，有mistral-7b
* https://codesphere.com
* https://github.com/zszazi/Deep-learning-in-cloud
* https://lightning.ai/ 22小时
* https://studiolab.sagemaker.aws/ 需要waitlist

## whisper.ai

* OpenAI出的，2/3的训练集是英文，剩下的是多语言
* 单次只支持30秒
* whisper.cpp：llama.cpp的作者出的
* TODO: qwen-audio
* https://github.com/Purfview/whisper-standalone-win

## VSC编码插件

* https://github.com/continuedev/continue
* https://github.com/rjmacarthy/twinny
* https://github.com/Pythagora-io/gpt-pilot

## 资源收集

* https://dang.ai/
* https://www.huntagi.com/


TODO:
去掉bnb
torch.compile()支持几种模式，默认简单编译。如果加强编译最好保存模型。要看如何保存。不知道对比bettertransformer是兼容还是哪个好。https://huggingface.co/docs/transformers/main/en/perf_torch_compile
use_cache=True
