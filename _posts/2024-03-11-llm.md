# LLM

* FP32：就是普通的float。FP16：精度稍差，数字范围变为±65000。BF16：精度更差，但保留了FP32的数字范围，只有3060后才支持，IntelXeon3rd也支持。混合精度：平常用FP16，在加法用32。TF32：只有A100支持
  * torch_dtype：pytorch在不指定时会默认转换到float32，预训练模型应指定auto，否则可用torch.bfloat16。查看类型：next(model.parameters()).dtype或pipe.torch_dtype
  * 大多数游戏GPU，FP64的性能是FP32的1/32

## 模型

* 排行榜
  * 英文
    * https://hf.co/spaces/open_llm_leaderboard/open_llm_leaderboard 可以筛选大小
    * 包含私有模型的榜：https://chat.lmsys.org/?leaderboard
    * https://www.vellum.ai/llm-leaderboard
  * 中文：https://cevalbenchmark.com/static/leaderboard_zh.html https://www.superclueai.com/
  * 代码：https://paperswithcode.com/sota/code-generation-on-humaneval
  * https://ollama.com/library 选MostPopular
* 英文：llama mistral gemma。小模型：phi
  * 超小：MBZUAI/LaMini-Flan-T5-77M 微调自谷歌但不是CausalLM、apple/OpenELM-270M-Instruct（tokenizer用的llama2的，用起来麻烦）
* 中文：Qwen2、GLM4（9B很慢）。XuanYuan-6B、BlueLM-7B、Baichuan2-7B-Chat、Yi-6B。小模型：MiniCPM-2B
* 代码：deepseek-coder、dolphin-2_6-phi-2（dolphin基于starcoder2）。Code-Qwen
* 选好后找对应的GGUF版，下Q5_K_M或Q4_K_M，主要靠 https://hf.co/TheBloke
* 原始模型只是续写。Instruct版每个模型有不同的prompt模板，主流是ChatML

## GUI

* https://jan.ai/ 开源桌面版本。导入后在Model和Assistant的三个点处修改全局设置
* text-generation-webui
* https://github.com/h2oai/h2o-llmstudio 开发比较初期但有公司支持
* https://chatboxai.app/
* 闭源：https://lmstudio.ai/ https://faraday.dev/ https://msty.app/
* gpt4all：不能选本地下载的
* https://github.com/lobehub/lobe-chat/blob/main/README.zh-CN.md
* https://github.com/lencx/Noi
* https://github.com/open-webui/open-webui
* 以知识库为特点：dify、MaxKB、AnythingLLM、RAGFlow、FastGPT

## PromptEngineering

* https://platform.openai.com/examples
* https://github.com/dair-ai/Prompt-Engineering-Guide
* https://github.com/f/awesome-chatgpt-prompts https://github.com/PlexPt/awesome-chatgpt-prompts-zh
* https://learnprompting.org/zh-Hans/docs/intro
* https://www.promptingguide.ai/zh
* https://github.com/f/awesome-chatgpt-prompts
* In the context of xxx, 实际问题
* 区分指令和内容：Summarize the text **delimited by triple backticks** into a single sentence
* 结构化输出：Provide content in JSON format with the following keys: aaa, bbb
* 检查条件：If the text contains xxx, action. If not, simply write \"No info provided.\"
* few shot：Your task is to answer in a consistent style. <角色A>: xxx. <角色2>: xxx

## 开发框架

* https://github.com/mudler/LocalAI
* Langchain：工具包，包括RAG、Agent等
  * langchain-huggingface：https://huggingface.co/blog/zh/langchain
* RAG检索增强生成能力：获取外部数据解决幻觉、无法获取实时更新、私有数据
  * quivr、llamaindex
  * 其实是先用外部向量数据库处理自己数据，提问时先从数据库里取出相关内容附加到prompt里，不会更改参数

## transformers

* transformers accelerate(device_map，会自动选用cpu或cuda，且显存不够时会自动用内存) bitsandbytes(load_in_4bit)
* torch：linux版默认有cuda，Win默认没有。装cpu版：conda install pytorch-cpu或pip transformers[torch]
* datasets
  * load_dataset(ds_id, split=train、'validation[:10]')、ds['train'、'validation']
* evaluate
* 加速
  * Flash Attention 2(flash-attn)：attn_implementation="flash_attention_2"。仅支持GPU，不一定支持Win。宣传说如果支持则没有理由不用；不清楚是否仅加速推理
    * Pytorch2.2的SDPA集成了2。transformers目前会在可用时自动用。optimum.BetterTransformer没啥用了
  * ExLlamaV2：仅支持GPU
  * intel-extension-for-pytorch：仅linux或WSL2
  * intel-extension-for-transformers.transformers import AutoModelForXXX。可能还要安装neural-speed作为推理后端
  * m=torch.compile(m)：支持几种模式，默认reduce-overhead简单编译，max-autotune最好保存模型

### 推理

* pipeline支持的任务：https://hf.co/docs/transformers/main/zh/task_summary
  * text-generation：又叫Causal Language Modeling。包括聊天机器、完成句子、内容生成。是decoder-only架构，包括GPT、PaLM、BLOOM
  * Text-to-Text Generation：又叫Sequence-to-Sequence Modeling。包括翻译、问答、总结、自动更正。是encoder-decoder架构，包括T5、BART

```py
from transformers import pipeline

model_id = '本地模型文件夹' # 也称为checkpoint
pipe = pipeline('text-generation', model_id, torch_dtype='auto', device_map='auto', load_in_4bit=True) # 可任意指定其它参数，会应用到后续调用中
# pipe()的第一个参数是inputdata；也能接受迭代器返回迭代器，此时一般还调整batch_size

user_input = 'Who are you'
message = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': user_input}
]

outputs = pipe(message, max_new_tokens=256,
    do_sample=True, temperature=0.6, top_p=0.9,
)

o = outputs[0]['generated_text'][-1]['content']

# 继续对话
chat = outputs[0]['generated_text']
chat.append({'role': 'user', 'content': user_input})

# 手动创建使用
model = AutoModelForCausalLM.from_pretrained()
tokenizer = AutoTokenizer.from_pretrained()
formatted_chat = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(formatted_chat, return_tensors='pt', add_special_tokens=False).to('cuda')
outputs = model.generate(**inputs, max_new_tokens=99)
decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)

# 界面
import gradio as gr
gr.Interface.from_pipeline(pipe).launch()
```

## 量化

* 分类
  * 训练后量化(Post Training Quantization, PTQ)：量化预训练好的模型
  * 量化感知训练(Quantization Aware Training, QAT)：在训练或进一步微调之前执行量化。如QLoRA（就是量化+LoRA）
    * 目前的量化算法基本都能配合PEFT。但quanto除外
  * 有一种说法是QAT需要大量训练成本，对于LLM不能接受
* 效果：AWQ > GPTQ > bitsandbytes(bnb)
* GPTQ(AutoGPTQ, auto-gptq)采用int4/fp16 (W4A16)的混合量化方案，模型权重被量化为int4，而激活值则保留在float16。在推理阶段，模型权重被动态地反量化回float16。也支持int8
* AWQ：仅有int4，torch_dtype仅为float16。推理时最好使用AutoAWQForCausalLM.from_quantized()，具有合适的默认参数。支持CPU推理，pypi安装的默认是GPU的
* bnb：可用于非文本（多模态）模型，可在线量化（On the fly）无需先单独转换（但也可以保存）。简单，零样本
* optimum-quanto：HF出的，兼容torch.compile，支持cv模型，支持在线量化，不能用transformers保存。transformers中集成的貌似是老版本的，不要看它的文档。实测会优先用VC，不存在时会用回落实现
* TODO:optimum-intel
* intel/auto-round：自己宣传比awq好

## 微调

* 按规模：PEFT(Parameter Efficient Fine Tuning)低参微调，Full Parameter Fine Tuning全参微调
* 按训练流程：SFT(Supervised Fine Tuning)有监督微调，IT(Instruction Tuning)指令微调是SFT的特殊形式，RLHF(Reinforcemnet Learning Human Feedback)人类反馈强化学习，包括DPO直接策略优化，PPO近端策略优化。不改变参数：In-Context Learning。SFT和RLHF又称“对齐Alignment”
* PEFT算法：LoRA、Prefix Tuning(P-Tuning v2)
* 教程：https://hf.co/docs/transformers/main/zh/training https://datasciencedojo.com/blog/fine-tuning-llms/ https://www.kaggle.com/code/hhoang41/llama-2-fine-tuning-using-qlora https://github.com/huggingface/transformers/issues/31332 https://generativeai.pub/a-beginners-guide-to-fine-tuning-mixtral-instruct-model-7f6a30aacf61 https://discuss.huggingface.co/t/fine-tune-and-then-successfully-awq-quantize/73239 https://gist.github.com/younesbelkada/f48af54c74ba6a39a7ae4fd777e72fe8
* 工具框架
  * LLaMA-Factory、unsloth、peft(hf出的, LoraConfig, PeftModel)
  * LoRA能单独输出微调的部分，之后与基础的合并，方便已有基础镜像的人快速获得微调后的
  * axolotl：简化微调流程
  * TRL：一站式解决SFT(hf，SFTTrainer接受LoraConfig)、RM(Reward Modeling)、PPO，能自动将数据集格式化为chatml格式
  * torchtune：pytorch官方的微调框架，但目前只支持llama mistral gemma，7B+QLoRA消耗8.5G VRAM

```py
from peft import get_peft_model, LoraConfig, TaskType
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
model = get_peft_model(model, peft_config)
训练
model.save_pretrained("output_dir") # 只会保存经过训练的增量PEFT权重

# 加载增量权重用于推理
from peft import PeftModel, PeftConfig
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForXXX.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
```

## 其它流程

* 预训练：无标注、无监督，大量token，“下一个单词预测”任务
  * https://arxiv.org/abs/2212.14034
* 部署：vllm SkyPilot LightLLM openvino huggingface/text-generation-inference
* 加速：DeepSpeed
* Agent：https://github.com/Significant-Gravitas/AutoGPT
* 单机训练LLM：https://github.com/hpcaitech/ColossalAI
* 教程：https://github.com/mlabonne/llm-course
* 利用自然语言生成项目：https://github.com/gpt-engineer-org/gpt-engineer

## HF Hub

* 下载的模型默认储存到 ~/.cache/huggingface/hub。更改：HF_HUB_CACHE
* 用CLI下载：huggingface-cli download model_id --local-dir 本地文件夹。使用镜像：HF_ENDPOINT。也有py库；默认断点续传
* 带宽大时更换下载内核：huggingface_hub[hf_transfer]、HF_HUB_ENABLE_HF_TRANSFER=1
* HF_HUB_OFFLINE=1

### HF镜像

* https://hf-mirror.com
* https://modelscope.cn/models
* https://aliendao.cn
* https://aifasthub.com/
* https://ai.gitee.com/ https://gitee.com/hf-models

## llama.cpp

* 纯CPU推理，下avx2版本
* 运行：./main -m 模型.gguf -p "prompt"
  * 交互模式：--interactive-first -r "[User]" -p "[System]You are a helpful assistant." --in-prefix "[User]" --in-suffix "[AI]"
  * -n要输出的token数，默认-1无限适合Chat
  * -c context length不同模型不同，默认512，llama2可用4096
    * --keep 保留最初的token数
  * --color 用不同颜色区分用户输入和模型输出
  * -t 要使用的线程，似乎默认等于当前CPU线程数且最大10，推荐改为物理核心数
  * --prompt-cache cache.bin 缓存最初的状态加快启动
  * -f 从文件中读取prompt
  * --temp 0.8、--top-p 0.9 【默】调小会更保守更有确定性，调大会有创意更发散。一般只调其中一个。例如对于答案是唯一确定的场合（翻译）temp用0-0.3，写文章用0.5，写广告用0.7。OpenAI具有frequency_penalty和presence_penalty，默认为0；后者是一次性的，增大则会减少输出已存在过的词的概率，有助于讨论新主题；前者惩罚频繁使用的词，但仍可能从AAA变到ABABAB
  * --repeat-penalty 1.0即禁用 --no-penalize-nl 作者说对于Chat模型不应启用重复惩罚
* example目录、prompts目录
* llama-bench：测试512个长度的提示词的处理时间、生成128个token所需时间
* 相关项目
  * 对用户更友好的封装：ollama run。可以自动利用显卡。llama.cpp也可以但要手动指定 -ngl 层数 控制把多少层放到GPU里。但实际上即使只有1层放到内存里也会造成重大性能损失
  * 与模型一起封装成可执行的单文件：https://github.com/Mozilla-Ocho/llamafile
  * 另一款仅支持HF模型的聊天工具：https://github.com/foldl/chatllm.cpp
* CPU微调：https://rentry.org/cpu-lora
  * 可以微调具有llama架构的gguf
* 转换模型到GGUF：先转换成FP16，再量化，看docs/HOWTO-add-model.md。未测试：convert.py BaseModel --outtype F16 --outfile Base.gguf --concurrency N; quantize.exe Base.gguf Out.Q5_K_M.gguf Q5_K_M
* transformers也能加载gguf，会先解量化到fp32，再转换到pytorch的权重类型

## 免费GPU资源

* Google Colab：没有具体的说明，但是有免费的
* kaggle：NVIDIA TESLA P100 GPU 30 hours/week
* cloudflare ai：10,000 Neurons per day 只有预设的几个模型，有mistral-7b
* https://codesphere.com
* https://github.com/zszazi/Deep-learning-in-cloud
* https://lightning.ai/ 22小时
* https://studiolab.sagemaker.aws/ 需要waitlist
* https://huggingface.co/spaces/zero-gpu-explorers/README

## whisper.ai

* OpenAI出的，2/3的训练集是英文，剩下的是多语言
* 单次只支持30秒
* whisper.cpp：llama.cpp的作者出的
* TODO: qwen-audio
* https://github.com/Purfview/whisper-standalone-win

## VSC编码插件

* https://github.com/continuedev/continue
* https://github.com/rjmacarthy/twinny
* https://github.com/Pythagora-io/gpt-pilot

## 资源收集

* https://dang.ai/
* https://www.huntagi.com/


TODO:
use_cache=True 好像默认就是？训练时要关？
AutoModel加载后模型默认为推理模式。不知道peft会不会切换
