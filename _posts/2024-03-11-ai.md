# LLM

* FP32：就是普通的float。FP16：精度稍差，数字范围变为±65000。BF16：精度更差，但保留了FP32的数字范围，只有3060后才支持。混合精度：平常用FP16，在加法时用32

## 模型

* LLaMA系榜单：https://ollama.com/library 选MostPopular
* 排行榜
  * 英文：https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard 可以筛选大小。https://www.vellum.ai/llm-leaderboard
  * 中文：https://cevalbenchmark.com/static/leaderboard_zh.html https://www.superclueai.com/
  * 代码：https://paperswithcode.com/sota/code-generation-on-humaneval
* 英文（应选基于它们的微调版）：Mistral-7B-Instruct-v0.2（mistral-openorca还没更新，现在有Mistral-7B-Instruct-v0.2-DARE，ctx窗口32k）。grok-1 推特出的，314B。小模型：phi-2
* 中文：XuanYuan-6B、BlueLM-7B、Qwen1.5-7B、Baichuan2-7B-Chat、Yi-6B、Chinese-LLaMA-Alpaca-2（1较差，2无测评）。小模型：MiniCPM-2B
* 代码：deepseek-coder、dolphin-2_6-phi-2（dolphin基于starcoder2）。Code-Qwen还没发布
* 较差：tinyllama、gemma
* 选好后找其他人转换的GGUF版本，下Q5_K_M或Q4_K_M，主要靠 https://huggingface.co/TheBloke
* 原始模型只是续写。Instruct版每个模型有不同的prompt模板，不过基本分为ChatML和Alpaca两种。Chat版无需模板

## GUI

* https://jan.ai/ 开源桌面版本。导入后在Model和Assistant的三个点处修改全局设置
* text-generation-webui
* https://github.com/h2oai/h2o-llmstudio 开发比较初期但有公司支持
* https://lmstudio.ai/ https://faraday.dev/ https://msty.app/：闭源
* gpt4all：不能选本地下载的

## llama.cpp

* 纯CPU推理，下avx2版本
* python绑定提供py api和作为兼容OpenAI API的WebServer。或用https://github.com/mudler/LocalAI
* 运行：./main -m 模型.gguf -p "prompt"
* 交互模式：--interactive-first -r "[User]" -p "[System]You are a helpful assistant." --in-prefix "[User]" --in-suffix "[AI]"
* -n要输出的token数，默认-1无限适合Chat
* -c context length不同模型不同，默认512，llama2可用4096
  * --keep 保留最初的token数
* --color 用不同颜色区分用户输入和模型输出
* -t 要使用的线程，似乎默认等于当前CPU线程数且最大10，推荐改为物理核心数
* --prompt-cache cache.bin 缓存最初的状态加快启动
* -f 从文件中读取prompt
* --temp 0.8、--top-p 0.9 【默】调小会更保守更有确定性，调大会有创意更发散。一般只调其中一个。例如对于答案是唯一确定的场合（翻译）temp用0-0.3，写文章用0.5，写广告用0.7
* --repeat-penalty 1.0即禁用 --no-penalize-nl 作者说对于Chat模型不应启用重复惩罚
* example目录、prompts目录
* llama-bench：先测试512个长度的提示词的处理时间，再测试生成128个token所需时间
* 对用户更友好的封装：ollama run。可以自动利用显卡。llama.cpp也可以但要手动指定 -ngl 层数 控制把多少层放到GPU里。但实际上即使只有1层放到内存里也会造成重大性能损失
* 与模型一起封装成可执行的单文件：https://github.com/Mozilla-Ocho/llamafile
* 另一款支持HF模型的聊天工具：https://github.com/foldl/chatllm.cpp

## PromptEngineering

* https://platform.openai.com/examples
* https://github.com/dair-ai/Prompt-Engineering-Guide
* https://github.com/f/awesome-chatgpt-prompts https://github.com/PlexPt/awesome-chatgpt-prompts-zh
* https://learnprompting.org/zh-Hans/docs/intro
* https://www.promptingguide.ai/zh
* https://github.com/f/awesome-chatgpt-prompts
* In the context of xxx, 实际问题

## 微调

* 工具：LLaMA-Factory、QLoRA适合小内存
  * 一般能单独输出微调的部分，之后与基础的合并，方便已有基础镜像的人快速获得微调后的
* 按规模：PEFT(Parameter Efficient Fine Tuning)低参微调，Full Parameter Fine Tuning全参微调
* 按训练流程：SFT(Supervised Fine Tuning)有监督微调，IT(Instruction Tuning)指令微调是SFT的特殊形式，RLHF(Reinforcemnet Learning Human Feedback)人类反馈强化学习，包括DPO直接策略优化，PPO近端策略优化。不改变参数：In-Context Learning。SFT和RLHF又称“对齐Alignment”
* RAG检索增强生成能力：获取外部数据解决幻觉、无法获取实时更新、私有数据。处理框架：quivr、llamaindex、dify。其实是先用外部向量数据库处理自己数据，提问时先从数据库里取出相关内容附加到prompt里，不会更改参数
* https://datasciencedojo.com/blog/fine-tuning-llms/
* Langchain：工具包，包括RAG、Agent等

## 其它流程

* 预训练：无标注、无监督，大量token，“下一个单词预测”任务
* 量化：AutoGPTQ AutoAWQ
* 部署：vllm SkyPilot
* Agent：AutoGPT
* 单机训练LLM：https://github.com/hpcaitech/ColossalAI

## HF镜像

* https://hf-mirror.com
* https://modelscope.cn/models
* https://aliendao.cn
* https://gitee.com/modelee
* https://aifasthub.com/

## 免费GPU资源

* colab：没有具体的说明，但是有免费的
* kaggle：NVIDIA TESLA P100 GPU 30 hours/week
* cloudflare ai：10,000 Neurons per day 只有预设的几个模型，有mistral-7b
* https://codesphere.com
* https://github.com/zszazi/Deep-learning-in-cloud

## 资源收集

* https://dang.ai/
* https://www.huntagi.com/
