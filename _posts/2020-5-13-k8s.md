---
title: Kubernetes
---

## 理论

* 在计算集群中（不是mysql这种，而是短生命周期消耗大量资源的），我们通常希望OOM的时候直接杀掉进程，执行故障转移，把进程在其他节点上重启起来，因此选择关闭swap

## [k3s](https://docs.k3s.io/zh/)

* 修改自启命令在/etc/systemd/system/k3s.service，数据默认在/var/lib/rancher/k3s或${HOME}/.rancher/k3s中
* 默认安装的是server模式，当提供K3S_URL时则变为agent模式；server默认同时也会在本机安装agent；自己指定用`INSTALL_K3S_EXEC='server --xxx`
* server会自动创建kubectl, crictl, ctr, k3s-killall.sh, k3s-uninstall.sh；agent有k3s-agent-uninstall.sh
* 实验性选项--rootless，--https-listen-port默认6443，--tls-san可指定自定义server域名
* 默认会用sqlite；server也可以是集群，形成HA嵌入式DB：master安装时用K3S_CLUSTER_INIT，加入时用-s=url（或用K3S_URL且exec指定为server），但不知道要不要用token？官网教程没用
* 升级后需要systemctl restart k3s
* The Traefik ingress controller will use ports 80, 443, and 8080 on the host。关键是它和nginx都不报错，现象是是curl localhost的80和443卡住，而且退出k3s后也不会恢复。用--disable=traefik可以解决但还是要自己配ingress
* 安装后会在kube-system命名空间里运行一些pod，或可用--all-namespaces查看
* kubeconfig在`/etc/rancher/k3s/k3s.yaml`中，使用前要把里面的localhost改为server的host

```bash
# server。latest为版本号最大的pre-release，默认是stable
curl -fsSL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -

# agent。token在server的/var/lib/rancher/k3s/server/node-token中，也可用K3S_TOKEN_FILE
curl ... | K3S_TOKEN=xxx K3S_URL=https://serverip:6443 K3S_NODE_NAME=xxx sh -

# 二进制文件：https://github.com/rancher/k3s/releases
k3s server &
k3s agent -s https://serverip:6443 -t ${NODE_TOKEN} &
```

## [kubectl](https://kubernetes.io/zh/docs/reference/kubectl/overview)

* 官方版本安装包：https://kubernetes.io/zh/docs/tasks/tools/install-kubectl/

```bash
alias k=kubectl
alias docker=crictl
touch ~/.kube/config && chmod 600 ~/.kube/config
source <(kubectl completion bash)

临时修改kubeconfig：export KUBECONFIG=$(pwd)/kubeconfig
合并kubeconfig：KUBECONFIG=~/.kube/config:new-config k config view --merge --flatten > ~/.kube/config

k get xxx -o wide可以得到更多的信息，-o json以json格式输出，-o yaml --export可以获取该资源的配置文件
同时指定多个资源用逗号且无空格
-v=7可以显示包括联网在内的详细信息
-n 在指令空间中进行操作；-l过滤标签，如-l "key=value,key=value"、-l "key1 in (val1,val2),!key2"
k --all-namespaces=true get all; k delete all --all # 但all无法获取PVC
```

### 命令

* 有的命令加不加s都一样，logs除外

```bash
k create/get/delete node/deploy/ns/issuer/pod/service/componentstatus/service/certificates <资源的name>
k apply -f xx.yml/<-R -f .>/<-f ->；get那些也可用
k config current-context/get-contexts/use-context/view name [nsname]
k version/cluster-info # 前者查看kubectl和server的版本，后者查看master和KubeDNS的url
k logs [-f] podname或-l lablename
k describe 资源类型/资源名 # 显示已部署的资源的信息，其中资源名可省
k explain <资源类型> # 类似于文档，会显示API版本
k exec type/xxx -- cmd arg # 还可以-it；run中也可以，注意也要加-- bash；但我实际用了会卡住
k run ngx --image nginx; k expose pod ngx --port=80 --nodeport=30000 --type=NodePort; # 直接运行nginx pod
k scale deployment --replicas 2 <name>
k create configmap <mapname> --from-file=xxx (或=key-name=path，可多次使用或指定目录但忽略子目录); k get configmap <name> -o yaml
k proxy --port=xxx # 启动API网关，之后可以用curl localhost:8080/api/v1/namespaces/ 等获取信息
k port-forward svc/xxx 8080:80 # 不清楚如何使用还有和export的区别
k api-versions # 查看支持的API版本
k label/annotate <type/xxx> key=value # 设定label和注解

k apply ... --record=true; k rollout status deploy xxx; k rollout history deply xxx; k rollout undo deploy xxx --to-revision=1
k patch deployment deployment-demo -p '{"spec": {"minReadySeconds": 5}}'(-p 以补丁形式更新补丁形式默认是json)
k set image deployments deployment-demo myapp=ikubernetes/myapp:v2 修改depolyment中的镜像文件
k rollout status deployment deployment-demo 打印滚动更新过程中的状态信息
k get deployments deployment-demo --watch 监控deployment的更新过程
k rollout pause deployments deployment-demo 暂停更新
k rollout resume deployments deployment-demo 继续更新
k rollout history deployments deployment-demo 查看历史版本(能查到具体的历史需要在apply的时候加上--record参数)
k rollout undo deployments deployment-demo --to-revision=2 回滚到指定版本，不加--to-version则回滚到上一个版本
```

## 杂项概念

* Pod：一个pod可以含有多个紧密配合的container，一个容器仍只运行一个进程，sidecar容器为主容器提供服务，如日志监视、HTTP反代。因为那些容器在一个pod里，它们一定会在同一个Node上。文件共享通信用volume，网络通信用localhost
* K8s在每个Pod启动时，会自动创建一个镜像为gcr.io/google_containers/pause:version的容器，所有处于该Pod中的容器在启动时都会添加诸如--net=container:pause --ipc=contianer:pause --pid=container:pause的启动参数，因此pause容器成为Pod内共享命名空间的基础。所有容器共享pause容器的IP地址，也被称为Pod IP。Pod IP是由flannel来分配的
* 一般来说，Pod不会自动消失，只能手动销毁或者被预先定义好的controller销毁。总体上来说，K8s拥有三种类型的controller：
  * Job：通常用于管理一定会结束的Pod。如果希望Pod被Job controller管理，那么restartPolicy必须指定为OnFailure或Never
  * ReplicationController，ReplicaSet和Deployment：用于管理永远处于运行状态的Pod。如果希望Pod被此类controller管理，那么restartPolicy必须指定为Always
  * DaemonSet：它能够保证你的Pod在每一台Node都运行一个副本。比如有的Node上有特殊的硬件，想在所有合适的节点上运行Pod就使用此功能
* 很多有状态的程序都需要集群式的部署，意味着节点需要形成群组关系，每个节点需要一个唯一的ID(例如Kafka BrokerId, Zookeeper myid)来作为集群内部每个成员的标识，集群内节点之间进行内部通信时需要用到这些标识。此时需要用StatefulSet，还需要Headless Service（spec.clusterIP:None）

## YML配置

在只能是字符串的地方只写数字会报错，必须用引号括起来。

### Deployment(deploy)

```yaml
# 替代ReplicationController，创建Pod和ReplicaSet
apiVersion: apps/v1 # 已经是最新版本
kind: Deployment
metadata:
  name: nginx # 必须要有
  namespace: playground
spec:
  selector:
    matchLabels: # 对应template.metadata.labels。仅有这一种选择器
      app: nginx
  replicas: 1
  revisionHistoryLimit: 3 # 保留历史版本
  minReadySeconds: 5
  strategy:
    rollingUpdate:
      maxSurge: 25% # 可为1；最大额外可以存在的副本数
      maxUnavailable: 25% # 在更新过程中能够进入不可用状态的Pod的最大值
    type: RollingUpdate

  template: # 下面的内容相当于Pod.yml（apiVersion: v1），还需name
    metadata:
      labels: # 必须要有
        app: nginx # key也可以自定义
    spec:
      containers:
      - name: nginx
        image: nginxinc/nginx-unprivileged
        imagePullPolicy: Always/IfNotPresent # 省略时，如果image为latest或不带tag，就为Always，否则为IfNotPresent；所以一般省略就完了
        restartPolicy: Always/OnFailure/Never # Pod类型好像不行
        command: ["nginx", "xxx"] # 或args: []
        ports:
        - name: ngx-8080
          containerPort: 8080 # 容器端口，内部程序就监听8080
          protocol: TCP
        env:
        - name: ENV_NAME
          value: ENV_VAL
        - name: SPECIAL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config # 来源于哪个configMap
              key: special # configMap里的key
        - name: POD_NAME
          valueFrom:
            fieldRef: # 还有resourceFieldRef获取容器的资源请求和资源限制信息
              fieldPath: metadata.name
        envFrom: # 导入所有的kv对
        - secretRef:
            name: api
        - configMapRef:
            name: special-config
        volumeMounts:
        - name: nginx-config-file
          mountPath: "/etc/nginx/nginx.conf"
          readOnly: true
        volumes:
        - name: cacheVolume
          emptyDir: {} # 当Pod（不管任何原因）被删除时，emptyDir也同时会删除，但删除容器不影响
        - name: testVolume
          hostPath:
            path: /data
        resources: # 如果只限制而不请求，则请求的和限制的相同；如果只请求，则限制的值会是集群的默认值（kind: LimitRange）
          requests: # 启动所需的最少
            cpu: '2m'
            memory: '5Mi'
          limits: # 超过就炸
            cpu: '100m' # 可以为1
            memory: '64Mi'
        livenessProbe: # 健康检查，返回200-399之间则认为正常；还有个readinessProbe
          httpGet:
            path: /v1/health
            port: 443
            scheme: HTTPS
            httpHeaders:
            - name: Host
              value: api.examplecom.com
          initialDelaySeconds: 120
          periodSeconds: 30 # 检查间隔时间
          timeoutSeconds: 5 # 检测的超时时间
          successThreshold: 1 # 成功门槛，还有个failureThreshold
        hostAliases:
        - hostnames:
          - status.localhost
          ip: 127.0.0.1
        lifecycle:
          preStop:
            exec:
              command: ["sleep", "15"]
        schedulerName: default-scheduler
        terminationMessagePath: /dev/termination-log
        dnsPolicy: ClusterFirst

      volumes:
      - name: nginx-config-file
        configMap:
          name: nginx-config-file
      - name: ssl-volume
        secret:
          secretName: examplecom
      - name: volv
        persistentVolumeClaim:
          claimName: local-path-pvc

      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 30
      nodeSelector: # 指定在某个node上运行
        disktype: ssd # 可自定义，需要先打标签k label nodes <nodename> disktype=ssd
```

### Service(svc)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: playground
spec:
  type: ClusterIP(默，只能从集群内部访问，从外部还要配ingress)/NodePort/LoadBalancer（好像只用于云服务提供商的4层LB）/ExternalName（还需设定externalname，为实际服务的完全限定域名）
  selector: # 对应deployment/Pod.metadata.labels；如果没找到，K8Spin报503
    app: nginx
  ports:
  - protocol: TCP #【默】，还可以是UDP和SCTP，不能是HTTP
    port: 80 # k8s集群内部访问service的端口，通过clusterIP:port可以访问到某个service
    targetPort: 8080 # 对应containerPort（Pod的Port）；如果那个有name则可用命名过的，则要改时就无需改这里了；可不写，则和port相同
    nodePort: 30000 # 绑定0.0.0.0:30000，必须大于30000；之后可以用NodeIP:NodePort访问或在那个节点上用127.0.0.1访问
    name: nginx-80
  externalIPs: # 默认是none，当node有公网IP时可用eIP:port访问svc
  - xx.xx.xx.xx # 不能是127/8
  clusterIP: xxx/None # 默认自动分配地址，这样能手动指定
  sessionAffinity: ClinetIP # 使代理将来自同一个client ip的所有请求转发至同一个pod上
---
spec: # 集群外访问负载均衡
  type: LoadBalancer # 在NodePort基础上，因此还需设置nodePort，略

  externalIPs: #外部负载均衡
    - 172.30.10.4
    - 172.30.10.3
  externalTrafficPolicy: Cluster
status:
  loadBalancer:
    ingress:
    - {}
---
spec: # vpc内网负载均衡
  type: LoadBalancer

  loadBalancerIP: 172.30.12.33
  externalTrafficPolicy: Cluster

  status:
    loadBalancer:
      ingress:
      - ip: 172.30.12.33
```

### Ingress(ing)

```yaml
# 用于公开http(s)路由，否则要用NodePort或LoadBalancer。基本上就是一个七层LB
apiVersion: networking.k8s.io/v1beta1 # 旧版本为extensions/v1beta1
kind: Ingress
metadata:
  name: hello-world
  annotations:
    ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/issuer: example-issuer # k get issuer

spec:
  tls:
  - hosts:
    - example-host.k8s.cloud # 可自定义提供者白名单中的子域名
    secretName: hello-world-certificate
  rules:
  - host: example-host.k8s.cloud
    http:
      paths:
      - path: / # 可为/bar/*；可不写
        backend:
          serviceName: nginx # 发到该service的该端口上
          servicePort: 80

  backend: # 注意没有host，基本不可用
    serviceName: testsvc
    servicePort: 80
```

### KubeConfig

```yaml
apiVersion: v1
kind: Config

clusters:
- cluster:
    certificate-authority-data: xxx
    #certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
- cluster:
    insecure-skip-tls-verify: true
    server: https://5.6.7.8
  name: scratch

users:
- name: developer
  user:
    token: xxx
    client-certificate: fake-cert-file
    client-key-data: xxx
    #client-key: fake-key-file
- name: experimenter
  user:
    password: some-password
    username: exp

contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: scratch
    namespace: default
    user: experimenter
  name: exp-scratch

preferences: {}
current-context: ""
```

### ConfigMap(cm)

```yaml
# k apply -f conf.yml 此文件就作为一种资源放进去了
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config-file
  namespace: playground
  labels:
    app: nginx
data:
  config.json: |-
    {...}
```

### Secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque # base64编码格式的Secret，用来存储密码、密钥等
stringData:
  password: SECRET_PASSWORD_GOES_HERE
---
apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
type: kubernetes.io/tls
```

### PersistentVolumeClaim(pvc)

```
# 一种“储存请求”，提交后k8s会提供PersistentVolume；这样设计的原因是当PVC被删掉时可以控制是否删掉PV，默认不删
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-pvc
  labels:
    app: redis
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 100Mi
```

### NameSpace(ns)

在同一个namespace中不能创建同名资源。跨namespace访问Service：`servicename.nsname`

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: playground
  labels:
    name: playground
```

### CronJob

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: demo # Cronjob的名称
  labels:
    app: demo
    cron: hello
spec:
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: demo
            cron: hello
        spec:
          containers:
          - image: harbor.example.com/hello:v1
            imagePullPolicy: IfNotPresent
            name: hello
            command: [/bin/bash]
            args:
            - -c
            - echo "Hello World!!!" # job 具体执行的任务
            resources:
              limits:
                cpu: 300m
                memory: 300Mi
              requests:
                cpu: 100m
                memory: 100Mi
          nodeSelector:
            label: test
          imagePullSecrets:
            - name: harbor-certification
          restartPolicy: Never
  schedule: "15 * * * *"  # job执行的周期，cron格式的字符串
  successfulJobsHistoryLimit: 1
```

### Endpoints

当service没有selector时，可以手动创建一个将从此服务接收流量的Endpoints对象

```yaml
kind: Endpoints
apiVersion: v1

metadata:
  name: mongo

subsets:
  – addresses:
    – ip: 10.240.0.4
      ports:
      – port: 27017
```

### Certificate

```yaml
apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
  name: acme-crt
spec:
  secretName: acme-crt-secret
  dnsNames:
  - foo.example.com
  - bar.example.com
  acme:
    config:
    - http01:
        ingressClass: nginx
      domains:
      - foo.example.com
      - bar.example.com
  issuerRef:
    name: letsencrypt-prod
    # We can reference ClusterIssuers by changing the kind here.
    # The default value is Issuer (i.e. a locally namespaced Issuer)
    kind: Issuer
```

## K8s管理平台

* rancher，有中文文档，资源消耗较高
* kubesphere：国产的，才出不久，相对简单
* promtheus：容器平台内监控系统的事实标准，资源占用大
* https://kuboard.cn：国产
* https://platform9.com/pricing/ 非自己部署的，支持最多3个集群20个节点，注册要求business email；[评价企业级K8s平台的19个关键特性](https://platform9.com/wp-content/uploads/2018/11/A-Buyers-Guide-to-Enterprise-Kubernetes-Solutions.pdf )
* [k9s](https://zhuanlan.zhihu.com/p/143365635)：终端UI，管理k8s集群，kubectl的定位
* https://github.com/eip-work/kuboard-press UI

## K8s应用

* https://github.com/GoogleContainerTools/kaniko 在K8s中构建映像
* https://prometheus.io/ ：自动报警系统
* skaffold：CI

## TODO

* helm：相当于apt，能直接安装 https://hub.kubeapps.com/ 中的
* k8s知识图谱：https://www.processon.com/view/link/5ac64532e4b00dc8a02f05eb?spm=a2c4e.10696291.0.0.58d719a4dhb658
* 非官方中文文档：http://docs.kubernetes.org.cn/227.html http://docs.kubernetes.org.cn/227.html
* https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
* Volume相关：https://blog.csdn.net/watermelonbig/article/details/84108424
* https://zhuanlan.zhihu.com/c_1221763931397611520
* https://github.com/cnych/qikqiak.com/tree/master/content/post
* https://linfan1.gitbooks.io/kubernetes-chinese-docs/content/035-Pods.html 版本非常老，随便看看
* https://www.cnblogs.com/liufei1983/category/1369899.html 每天五分钟玩转k8s
* https://github.com/kelseyhightower/kubernetes-the-hard-way
* https://kubernetes.feisky.xyz/
* https://jimmysong.io/kubernetes-handbook/
* https://zhuanlan.zhihu.com/p/92923128 30分钟带你搭建一套Dashboard的kubernetes（K8S）集群；不过是RH的
* https://blog.rj-bai.com/post/152.html https://blog.rj-bai.com/post/151.html https://blog.rj-bai.com/post/161.html
* https://www.jianshu.com/p/78a5afd0c597 从零开始搭建Kubernetes集群
* Kubernetes in Action中文版
* https://zhuanlan.zhihu.com/p/109577847 30分钟就搭建好了Kubernetes 集群
* https://k8slens.dev/
* https://zhuanlan.zhihu.com/p/339008746
* https://jimmysong.io/envoy-handbook/ https://www.envoyproxy.io/

## 参考

* https://kubernetes.io/zh/docs/home/
* https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/
* https://rancher.com/docs/k3s/latest/en/
* https://zhuanlan.zhihu.com/p/65159435
* https://www.cnblogs.com/kevincaptain/p/9929717.html
* https://docs.kubesail.com
* https://docs.k8spin.cloud
* https://zhuanlan.zhihu.com/p/99135500
* https://www.jianshu.com/u/e6deef6e5882
* https://blog.csdn.net/kenkao/article/details/86764788
* https://zhuanlan.zhihu.com/p/85810571
