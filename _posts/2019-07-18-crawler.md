---
title: 爬虫
---

## Python

* https://scrapy.org/：比较重
* request就是个http请求库，bs4是个html解析库
* pyspider：国产有WebUI的爬虫

待读：

* https://github.com/lining0806/PythonSpiderNotes
* https://blog.csdn.net/cwyalpha/article/details/48111173
* https://github.com/iBug/douban-spider https://ibugone.com/blog/2019/12/mass-crawl-douban-with-aws/
* https://zhuanlan.zhihu.com/p/73742321 python爬虫100个入门项目
* https://zhuanlan.zhihu.com/pachong
* https://blog.csdn.net/wxg694175346/column/info/why-bug
* https://zhuanlan.zhihu.com/fxxkpython

### Requests

异步的（非官方）：https://github.com/ross/requests-futures

```python
import requests

s = requests.session()
s.headers.update({'Referer': referer})
s.cookies.RequestsCookieJar().set(k,v,domain,path)
requests.utils.add_dict_to_cookiejar(cj, cookie_dict)/cookiejar_from_dict/dict_from_cookiejar；不能直接用update！内容需要先strip一下

# url必须要有scheme
r: Response = requests.get(url,params={k:v})/post(url,data/json={k:v}) # get的params会自动变成查询参数，且值为None的不会附加上去；默认会自动跟随headers中的跳转
r.raise_for_status()、.json()
r.text（自动解码）、url、encoding（可赋值）、content（二进制，但会自动解码gzip）、raw为原始响应，且要设置stream=True、history、headers为响应头部，请求的用request.headers

#参数：
timeout=... 默认是无穷大，不加就可能失去响应。直接赋值只影响连接，控制下载要传元组
allow_redirects=False
verify=False


proxies = { # 支持socks，但要pip install requests[socks]
  "http": "http://10.10.1.10:3128",
  "https": "http://10.10.1.10:1080",
}
get(url,proxies=proxies)
```

### Beautiful Soup

```
pip install html5lib
from bs4 import BeautifulSoup, Tag

soup: Tag = BeautifulSoup(html, 'html.parser') # 'html5lib'
soup.select/select_one('css selector')
.name为tag名，根元素为[document]；.attrs为属性，也可以用字典的方式取
.contents相当于list(children)；descendants是所有子元素递归组合成一个生成器
.string取当前元素的文本；.strings递归取文本，.stripped_strings去掉空格和换行，但这两者是生成器；.text相当于join了的strings
直接.元素是递归地取第一个指定的子元素
.prettify()：略
其它具体可看：https://www.cnblogs.com/wcwnina/p/8093987.html
不清楚怎么识别编码的，应该是http头；已知meta的content charset是无效的。.encoding可以赋值为'gbk'或者用apparent_encoding
```

## 自动化的

* 造数：https://www.zaoshu.io
* https://www.webscraper.io

## 要解决的问题

流程：获取网页 -> 解析网页，提取需要数据 -> 存储

* 限制ip：构建好ip池，或者网上免费代理（西刺代理）
* headers：user agent：服务器使用这个判断你使用的浏览器；Referer：浏览器通过此来判断你从哪一个网页跳转过来
* Cookie：requests用session先拿到cookies
* ajax：f12分析请求规律，直接requests请求；通常需要带上`'X-Requested-With':'XMLHttpRequest'`
* 爬的太慢：多线程、scrapy异步、分布式
* 验证码、登录
* 乱码
* 爬WAP站
* robots协议
