## 消息队列

* 解耦（发送者和接收者不必了解对方）、异步（发送者无需等待消息被处理）。流量削峰、(生产者)消息重发、(消费者)重试消费、死信队列、优先级
* 队列是否允许丢东西？比如如果队列短时间挂掉，生产者是必须停止服务，还是继续服务但不再插入队列（这样就会丢东西），或者生产者可以在本地先暂时堆积一下，直到队列恢复工作？
* 消费者是否需要一个“commit”语义，表示处理完了一个事件？还是只要从队列里取出来就可以了，万一没处理也没所谓？
* 是否有事件重放的需要？比如上线了一个版本的消费者然后发现有bug，处理错了一些数据。修复后希望能重新处理之前出错的数据
* 如果consumer处理失败怎么处理？是直接丢弃，还是重新插入到队列中？
* 是否需要限制最大长度？如果到了最大长度，说明消费者跟不上生产者的速度，此时需要卡住生产者吗？
* 两类队列：业务事件队列，不能丢东西，可能需要exactly once语义，需要高可用。收集队列，用于数据分析，容忍数据延迟性比较大，但要求吞吐巨大，如日志
* 用途：分布式日志、流处理、发布者-订阅者消息

## 概念

* event(message)：包括metadata、key、value。其中key可以是任意的东西，也可以是null
* topic(log)：持久化到硬盘上的、不可变的、只能append的event集合
  * 读取时不会销毁，多个消费者可以互不干扰地从指定的offset读取
* partition分区：分布式的关键。一个topic划分为多个分区分布在多个broker上，不同的分区（对于一个消费者组来说）可以并行消费，只有一个分区则此topic不能并行消费
  * 生产者能决定将消息放到哪个分区上，一般如果有key就根据hash(key)%分区数，如果key为null则轮流放但同一批的会粘性发送
  * 一个分区里的消息是有序的。当分区数不变时，key相同，则一定放到相同的分区里
  * segment：一个分区保存在硬盘上的文件，相当于rolling log。默认满1GB或7天后更换
* consumer group消费者组
  * 消费者必须在消费者组中。一个消费者组语义上等于k8s根据同一个image创建出来的多个容器
  * 一条消息会广播给各个不同的消费者组。一个分区只会被各个消费者组里的一个消费者读取
  * 如果分区数大于一个消费者组里的消费者数，则一个消费者会读取多个分区，且会尽量平均分配。如果小于，则有消费者空闲
  * offset是下一次读取的地方，根据group.id和分区保存到集群中，没有“已发送但未确认队列”
    * 当某一个组的消费者为空超过默认7天则会删除此组
* replication副本：每个分区有一个leader多个follower。消息只在leader中读写，following只复制。当leader挂了后会从follower中重新选举
  * ISR(in-sync replicas)：保持同步了的节点集合，leader也算进去

### 消息传递语义

* at-most-once：无需副本、生产者无需重试(或acks=0)、消费者先commitSync再处理消息（不能是Async，否则可能重复消费）
* at-least-once：消息不会丢失，但可能被重复发送
* 生产者：默认retries>0且acks=all时会自动开启幂等性enable.idempotence（但librdkafka要手动开启），可以不产生重复记录，且重发的消息的的最终顺序保持了原本的顺序
* 消费者和自动提交
  * 默认情况下开始了自动提交。poll()后如果离上次提交超过了5秒，则提交当前offset。如果消息没处理就挂了，就丢了，此时业务上的语义是at-most-once；但若没到5秒，处理了消息后挂了，重启后又会重复处理消息
  * 推荐enable.auto.commit=false，poll处理完一批记录后commitAsync，在shutdown前commitSync。对于librdkafka，推荐开启自动提交但enable.auto.offset.store=false，用store_offsets()手动更新内存中的offset，再等poll提交到服务器，相当于commitAsync了
  * 用spring可不用调整autocommit
  * 如果消费者会输出到外部系统，要自己实现幂等性，一般如果消息里有主键，覆盖是没问题的，又称effectively-once
* exactly-once：消息不丢失也不重复。在分布式系统中exactly once delivery是不可能的，但可以exactly once processing。通过幂等性和事务(transactional.id、isolation.level=read_commited)实现，或者最好用Streams(processing.guarantee=exactly_once_v2)
* java的isolation.level默认为read_uncommited，librdkafka默认为read_commited

## 搭建

* win版可以用bin/windows下的bat。会在程序所在磁盘的根目录下创建/tmp文件夹，删除文件有问题会报错，基本不可用
* 升级集群：一次将一个节点关闭、更新代码、启动。集群都更新完后用kafka-features.sh upgrade --metadata 3.7
* Docker镜像：3.7后官方提供apache/kafka，之前有confluentinc/cp-kafka，后者可用环境变量设置配置如KAFKA_ADVERTISED_LISTENERS
* controller的数量要为2n+1个，能允许任何时候有n个不可用

```bash
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/kraft/server.properties 一个集群要相同的UUID
bin/kafka-server-start.sh 可选-daemon config/kraft/server.properties --override 配置k=v
```

### 服务器配置

```
node.id 每个实例都必须不一样，代替zookeeper时的broker.id
controller.quorum.voters 所有节点必须设为 所有控制节点id@ip:控制端口

listeners 一般意义上的监听，ip不写时为默认if，只有写0.0.0.0才是全部。控制节点要写CONTROLLER
advertised.listeners 只有broker要设置，controller不用。是客户端访问的地址，客户端连接本节点后，会又被要求去访问本项配置中的地址，与客户端和节点之间的网络环境有关
只在内网访问：listeners不改，advertised.listeners不设置，会自动与listener一样
只在外网访问：listeners不改，advertised.listeners设为公网ip。如果Kafka在容器中：如果客户端在另一个容器中，则用容器名（k8s用服务名），如用localhost则会访问客户端容器自身的；如果客户端不在容器中，则用PUBLISH的ip如localhost，不能用容器名因为无法解析
内外都访问：listeners监听两个端口且改自定义名字如INTERNEL和EXTERNEL，内部的用内网IP，advertised.listeners同理（如果不为控制结点则可以不填与listeners一致），listener.security.protocol.map将自定义名字设置验证方法如PLAINTEXT，inter.broker.listener.name设为内部的

log.dirs 默认在/tmp里，必须改，否则就被自动清理了
log.retention.hours 默认168即7天后删除。但只有“关闭”了(log.segment.bytes默认1G和log.roll.hours默认7天)的片段才会开始倒计时
log.cleanup.policy=compact 对于相同的key，会清理掉老的value。不会时刻清理，可以设置最小清理时间和最大清理时间，如小于最小时间则认为是活跃的会保留记录。还可按大小
num.partitions 默认的分区数量。default.replication.factor 默认的副本数量
auto.create.topics.enable 默认为true，给不存在的topic发数据时会自动创建，生产环境考虑设为false
min.insync.replicas 默认是1，leader自己也算进去，3个节点推荐设为2
最大消息大小：broker的message.max.bytes，topic的max.message.bytes（默认为broker的），生产者的max.request.size，默认1MB；消费者的fetch.max.bytes，默认50MB
```

### 加密和认证

* PLAINTEXT表示无认证无加密，SSL表示TLS双向证书验证和加密（也可像HTTPS只有单方证书，只加密无认证）
* SASL_PLAINTEXT表示只认证，传输无加密。SASL_SSL表示用SASL认证，SSL传输加密
* SASL有多种方式，其中用用户名密码的有PLAIN和SCRAM。PLAIN验证时会发送原始密码，需用SASL_SSL。SCRAM不会泄漏原始密码但也推荐配合TLS，可以动态增加和删除用户
* 储存在硬盘上的静态数据：没有内置加密方法。可用生产者端到端加密
* Broker之间的数据传输也可以加密和认证，如在不同局域网之间的replica，用双向SSL比较方便。若要用SCRAM，要在format时就--add-scram

```
服务端SSL：
security.protocol=SSL
ssl.keystore和truststore.type=默认JKS，可选PEM PKCS12
ssl.keystore.location=xxx.jks 如果用pem，必须带有私钥、签了名的证书、中间CA证书链
ssl.truststore.location=CA证书，所有节点用一样的，而keystore则是每个服务端和客户端有自己的私钥公钥
ssl.keystore.password、ssl.truststore.password。PEM用ssl.key.password，有人说无密码也要设为空
ssl.client.auth 默认none不启用客户端双向验证，若要启用则改为required
ssl.keystore.certificate.chain、ssl.keystore.key：用来直接填PEM格式证书字符串内容的，不用

服务端SASL：
sasl.enabled.mechanisms=PLAIN或SCRAM-SHA-512
listener.name.监听器名字小写.plain或scram-sha-512.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule或scram.ScramLoginModule required \
   username="admin" password="admin-secret" # 用于broker之间的认证
   user_admin="admin-secret" user_alice="alice-secret"; # 仅限PLAIN，创建了admin和alice两个账号。SCRAM另有方式创建，更麻烦

客户端：
security.protocol
SSL如果没有双向验证，只要设置truststore即可
librdkafka用ssl.ca.location。双向验证用ssl.key和certificate.location和password。对于PKCS12可以和JAVA的一样
ssl.endpoint.identification.algorithm=（空）禁用主机名验证。librdkafka用none
enable.ssl.certificate.verification= 仅限librdkafka，设为false后连truststore也无需设置了

SASL：
sasl.mechanism=PLAIN 或 SCRAM-SHA-512
JAVA：sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule或scram.ScramLoginModule required \
username="alice" password="alice-secret";
librdkafka：sasl.username、password
```

### 生产者配置

```
buffer，实时性和小包数量的trade-off：batch.size默认16KB，当消息满足此大小后会立即发出，否则再等linger.ms（默认0ms）后立即发出
compression.type=lz4或zstd  服务器也有同名配置，且默认值为producer字面量，表示保留生产者的配置。一般应由生产者压缩，否则无法节省网络流量。应选择适量的buffer否则压缩效果不好。不适合小数据、实时数据、加密后的高熵数据，适合JSON XML
acks=0/1/all(-1) 默认all表示等min.insync.replicas数量的follower同步完再返回，1表示leader完成即可
retries 默认是一个很大的值可认为次数无限，实际发送取决于delivery.timeout.ms默认2分钟
```

### 消费者配置

```
group.id 必须设置。Connector默认为connect-cluster，spring的stream默认为${spring.application.name}，不要和它们重名
auto.offset.reset 没有offset时从哪读，默认latest。设为none表示如果当前消费者没有offset记录则报错
max.poll.records 默认500，影响poll()，底层fetch时会自动缓存更多。要保证每批在max.poll.interval.ms(默认5分钟)内处理完，否则会被踢出组
fetch.min.bytes、fetch.max.wait.ms(librdkafka是fetch.wait.max.ms) 调大后一次性会多fetch一些，当重启后会重传更多数据，但对offset无影响

生产者和消费者都推荐设置client.id，如为hostname
```

## 客户端

### cli

```bash
所有命令行工具：https://docs.confluent.io/kafka/operations-tools/kafka-tools.html
创建topic：bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092 只要填任意一个broker即可 --partitions 3 --replication-factor 2 --config topic级别的k=v
查看：--describe --topic quickstart-events、--list、--delete、--alter
设置了加密的：--producer或consumer.config client-ssl.properties
额外的JVM参数：KAFKA_OPTS
删除记录（通过移动topic的起始offset）：kafka-delete-records.sh 但必须配合一个json的配置文件
```

第三方CLI：

* https://github.com/edenhill/kcat 是最初的，但不更新了
* https://github.com/birdayz/kaf 比较好
* https://github.com/streamdal/plumber 支持多种消息系统
* https://github.com/deviceinsight/kafkactl https://github.com/kcctl/kcctl https://github.com/fgeller/kt

### java

```java
// org.apache.kafka:kafka-clients、kafka-streams
var p = new KafkaProducer(properties 或 Map.of(配置)); 配置项的Key预定义了ProducerConfig等枚举
p.send(new ProducerRecord<>(topic, val)或(topic, key, val), (meta, ex)->{处理结果}) 或 syncSend

消费者：
c.subscribe(List.of(topic));
while (true) {
    for(var record : c.poll(Duration.ofMillis(100))) {
        record.key()/value()
    }
}

Runtime.getRuntime().addShutdownHook(new Thread(() -> p/c.close()));
```

### spring

```java
// org.springframework.kafka:spring-kafka
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=myGroup

生产者：@Autowired KafkaTemplate<String, String> kt; 默认仅能用String
kt.send(topic, val) 或 (topic, key, val) 返回 Future 可以.whenComplete((result, ex)->{if(ex == null)等待结果else处理异常})

消费者：不依赖KafkaTemplate
@Component 中 @KafkaListener(topics = "topic1,t2") void processMessage(String content, ConsumerRecordMetadata meta) {}
或在类上加@KafkaListener，方法上加@KafkaHandler，可有多个不同类型的重载，用于读取往一个topic里传了不同类型数据的情形

自定义类型：
spring.kafka.producer.value-serializer和consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonSerializer和JsonDeserializer JSON能处理整数小数和任何对象
序列化时会把对象类型写进消息的Header里，反序列化时还要设定 spring.kafka.consumer.properties[spring.json.trusted.packages]=com.demo 信任白名单才会解析Header中指定的类型。或对于无类型信息的json用 spring.kafka.consumer.properties[spring.json.value.default.type]=com.demo.MyClass 不过最好写进Listener的properties里否则全局只能有一个了，其中k就是中括号内的
如果想忽略key可用org.apache.kafka.common.serialization.VoidSerializer

Stream：@EnableKafkaStreams 自动配置了StreamsBuilder，用它创建KStream

自动创建topic，若已存在不会重复创建：@Bean NewTopic t() {return TopicBuilder.name("t1").partitions().replicas().config(k,v).build();}。还可用KafkaAdmin.NewTopics一次创建多个

spring-kafka-test：@EmbeddedKafka创建测试用的broker

事务：spring.kafka.producer.transaction-id-prefix=kafka_tx. 设置后所有行为都必须在Spring事务中进行。一个事务中可以发送多次，如果一个失败了会回滚
```

### python

```py
from confluent-kafka import Producer, Consumer
conf = {'bootstrap.servers': 'host1:9092', ...}; p = Producer(conf)

for i in range(10):
    p.pull(0) # 等待event，在此时或flush时才会触发callback
    p.produce(topic, value, key, callback) # 异步方法，会立即返回。如果队列缓存满了，会立即报BufferError，但默认大小足够，如果本地堆积了考虑程序有问题
p.flush()

with closing(c):
    c.subscribe([topics])
    while True:
        msg = consumer.poll(timeout=1.0) # 也可无参调用或传-1无限等待，会自动心跳
        if msg is None: continue # poll超时，没收到消息
        if msg.error():
            raise KafkaException(msg.error())
        处理msg.key() value() 是bytes
        c.commit默认是Async的，传False变为Sync。poll只会返回一个msg，推荐累积一些offset后再提交
```

## 生态

* Kafka Streams：一些类似于LINQ功能的API，比生产消费者更高层的客户端API。聚合时存在状态，而KS将它存到了集群里而非只在本地，能fault-tolerent。https://www.youtube.com/playlist?list=PLa7VYi0yPIH35IrbJ7Y0U2YLrR9u4QO-s
* Kafka Connect：Source将数据从数据库中取到Kafka，Sink输出到数据库中。只需要写一些json配置，无需代码。相当于预先写好的生产者消费者。列表：https://www.confluent.io/product/connectors/
* ksqlDB：作为client，使用类SQL语法而不用写Java代码，就能使用类似于KafkaStream的功能。会从头开始聚合事件，保留一个最新状态的view
* Schema Registry：验证消息内容。包括JSON Schema、avro（java对象）、protobuf
* UI：https://www.kafkamagic.com/ https://www.conduktor.io/ https://github.com/provectus/kafka-ui
* TODO: https://zhuanlan.zhihu.com/p/446774729 https://www.zhihu.com/question/630965706
* 对比RabbitMQ：https://support.huaweicloud.com/productdesc-kafka/kafka_pd_0003.html
